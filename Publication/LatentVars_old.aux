\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{pearl_causality:_2000}
\citation{hernan_estimating_2006}
\citation{hernan_estimating_2006}
\citation{anandkumar_learning_2013}
\citation{damour_multi-cause_2019}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{introduction}{{1}{1}{Introduction}{section.1}{}}
\citation{pearl_causality:_2000}
\citation{friedman_being_2013}
\citation{elidan_discovering_2001}
\citation{friedman1997learning}
\citation{friedman1998bayesian}
\citation{elidan_discovering_2001}
\citation{silva_learning_2006}
\citation{elidan_ideal_2007}
\citation{elidan_ideal_2007}
\citation{shepherd_probability-scale_2016}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Notation\relax }}{2}{table.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:notation}{{1}{2}{Notation\relax }{table.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background And Notation}{2}{section.2}}
\newlabel{Background}{{2}{2}{Background And Notation}{section.2}{}}
\citation{damour_multi-cause_2019}
\@writefile{toc}{\contentsline {section}{\numberline {3}Algorithm}{3}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gaussian Graphical Models (GGMs)}{3}{subsection.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Graph $G_u$. $U$ influences the outcomes, $O$, and a number of predictors, $V$, confounding many of the $V_j \rightarrow O_k$ relationships. Gray nodes are affected by $U$.\relax }}{3}{figure.caption.3}}
\newlabel{fig:sampleGraph}{{1}{3}{Graph $G_u$. $U$ influences the outcomes, $O$, and a number of predictors, $V$, confounding many of the $V_j \rightarrow O_k$ relationships. Gray nodes are affected by $U$.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Graph $G$. With $U$ latent, the graph adjusts, introducing spurious edges.\relax }}{3}{figure.caption.3}}
\newlabel{fig:sampleGraphOnObservables}{{2}{3}{Graph $G$. With $U$ latent, the graph adjusts, introducing spurious edges.\relax }{figure.caption.3}{}}
\newlabel{eq:linearForm}{{3}{3}{Gaussian Graphical Models (GGMs)}{equation.3.3}{}}
\newlabel{eq:linearFormNoU}{{4}{3}{Gaussian Graphical Models (GGMs)}{equation.3.4}{}}
\newlabel{eq:residualColumn}{{5}{3}{Gaussian Graphical Models (GGMs)}{equation.3.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The training data frame (left) implies a matching residual data frame (right) once the joint distribution of all variables is specified via a graph and its parameterization\relax }}{3}{table.caption.4}}
\newlabel{tab:MatchingResiduals}{{2}{3}{The training data frame (left) implies a matching residual data frame (right) once the joint distribution of all variables is specified via a graph and its parameterization\relax }{table.caption.4}{}}
\citation{anandkumar_learning_2013}
\citation{elidan_ideal_2007}
\citation{rawlings_applied_1998}
\newlabel{eq:resPCA}{{7}{4}{Gaussian Graphical Models (GGMs)}{equation.3.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Confounding of outcomes}{4}{section.4}}
\newlabel{eq:VIF}{{8}{4}{Confounding of outcomes}{equation.4.8}{}}
\newlabel{eq:vifImprovement}{{9}{4}{Confounding of outcomes}{equation.4.9}{}}
\citation{koller_probabilistic_2009}
\citation{bareinboim_causal_2016}
\citation{anandkumar_learning_2013}
\citation{louizos_causal_2017}
\citation{karatzoglou_kernlab_2004}
\newlabel{eq:ceilingTheoremPrep}{{11}{5}{Confounding of outcomes}{equation.4.11}{}}
\newlabel{eq:ceilingTheorem}{{12}{5}{Confounding of outcomes}{equation.4.12}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Gaussian Graphical Models With Interactions}{5}{subsection.4.1}}
\newlabel{eq:residualColumnWithInteractions}{{13}{5}{Gaussian Graphical Models With Interactions}{equation.4.13}{}}
\citation{kraskov_estimating_2004}
\citation{shepherd_probability-scale_2016}
\citation{mackay_information_2003}
\citation{elidan_ideal_2007}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Generalization to nonlinear functions}{6}{subsection.4.2}}
\newlabel{eq:residualColumnRank}{{14}{6}{Generalization to nonlinear functions}{equation.4.14}{}}
\newlabel{resPcaGam}{{15}{6}{Generalization to nonlinear functions}{equation.4.15}{}}
\newlabel{eq:rankSetRelationship}{{16}{6}{Generalization to nonlinear functions}{equation.4.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Generalization to categorical variables}{6}{subsection.4.3}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Implementation}{6}{section.5}}
\citation{gavish_optimal_2014}
\citation{friedman_being_2013}
\citation{elidan_learning_2005}
\citation{bnlearn}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Learning $\mathaccentV {bar}016{U}$ from structure residuals via EM\relax }}{7}{algocf.1}}
\newlabel{alg:latentEM}{{1}{7}{Implementation}{algocf.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Inferring linearly optimal $\mathaccentV {bar}016{U}$ and assessing its cardinality by permutations\relax }}{7}{algocf.2}}
\newlabel{alg:linearPCA}{{2}{7}{Implementation}{algocf.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Numerical Demonstration}{7}{section.6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces True network. \relax }}{8}{figure.caption.5}}
\newlabel{fig_truenet}{{3}{8}{True network. \relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Estimated network when $U_1$ and $U_2$ are unobserved.\relax }}{8}{figure.caption.5}}
\newlabel{fig_missing}{{4}{8}{Estimated network when $U_1$ and $U_2$ are unobserved.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Estimated network at the last iteration of algorithm \ref  {alg:latentEM}.\relax }}{8}{figure.caption.5}}
\newlabel{fig_estnet_infered}{{5}{8}{Estimated network at the last iteration of algorithm \ref {alg:latentEM}.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces $R^2$ in the prediction of the latent variable from the selected principal components.\relax }}{8}{figure.caption.6}}
\newlabel{fig:latrecons}{{6}{8}{$R^2$ in the prediction of the latent variable from the selected principal components.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Error in coefficients as a function of the iterations.\relax }}{8}{figure.caption.6}}
\newlabel{fig:errorcoef}{{7}{8}{Error in coefficients as a function of the iterations.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusions and Future Directions}{8}{section.7}}
\citation{louizos_causal_2017}
\bibdata{LatentVars}
\bibstyle{plainnat}
