\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\citation{pearl_causality:_2000}
\citation{hernan_estimating_2006}
\citation{spirtes_causation_1993}
\citation{hernan_estimating_2006}
\citation{pearl_causality:_2000}
\citation{friedman_being_2013}
\citation{elidan_discovering_2001}
\HyPL@Entry{0<</S/D>>}
\providecommand \oddpage@label [2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{introduction}{{1}{1}{Introduction}{section.1}{}}
\citation{friedman1997learning}
\citation{friedman1998bayesian}
\citation{elidan_discovering_2001}
\citation{silva_learning_2006}
\citation{elidan_ideal_2007}
\citation{anandkumar_learning_2013}
\citation{damour_multi-cause_2019}
\citation{shepherd_probability-scale_2016}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background And Notation}{2}{section.2}}
\newlabel{Background}{{2}{2}{Background And Notation}{section.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Notation\relax }}{2}{table.caption.2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:notation}{{1}{2}{Notation\relax }{table.caption.2}{}}
\citation{damour_multi-cause_2019}
\citation{anandkumar_learning_2013}
\citation{elidan_ideal_2007}
\@writefile{toc}{\contentsline {section}{\numberline {3}Diagnosing Latent Confounding}{3}{section.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Graph $G_u$. $U$ influences the outcomes $O$, and a number of predictors $V$, confounding many of the $V_j \rightarrow O_k$ relationships. Gray nodes are affected by $U$.\relax }}{3}{figure.caption.3}}
\newlabel{fig:sampleGraph}{{1}{3}{Graph $G_u$. $U$ influences the outcomes $O$, and a number of predictors $V$, confounding many of the $V_j \rightarrow O_k$ relationships. Gray nodes are affected by $U$.\relax }{figure.caption.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Graph $G$. With $U$ latent, the graph adjusts, introducing spurious edges.\relax }}{3}{figure.caption.3}}
\newlabel{fig:sampleGraphOnObservables}{{2}{3}{Graph $G$. With $U$ latent, the graph adjusts, introducing spurious edges.\relax }{figure.caption.3}{}}
\newlabel{eq:linearForm}{{3}{3}{Diagnosing Latent Confounding}{equation.3.3}{}}
\newlabel{eq:linearFormNoU}{{4}{3}{Diagnosing Latent Confounding}{equation.3.4}{}}
\newlabel{eq:residualColumn}{{5}{3}{Diagnosing Latent Confounding}{equation.3.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces The training data frame (left) implies a matching residual data frame (right) once the joint distribution of all variables is specified via a graph and its parameterization\relax }}{3}{table.caption.4}}
\newlabel{tab:MatchingResiduals}{{2}{3}{The training data frame (left) implies a matching residual data frame (right) once the joint distribution of all variables is specified via a graph and its parameterization\relax }{table.caption.4}{}}
\newlabel{eq:resPCA}{{7}{3}{Diagnosing Latent Confounding}{equation.3.7}{}}
\citation{rawlings_applied_1998}
\citation{koller_probabilistic_2009}
\@writefile{toc}{\contentsline {section}{\numberline {4}Confounding of causal effects}{4}{section.4}}
\newlabel{eq:VIF}{{8}{4}{Confounding of causal effects}{equation.4.8}{}}
\newlabel{eq:vifImprovement}{{9}{4}{Confounding of causal effects}{equation.4.9}{}}
\newlabel{eq:ceilingTheoremPrep}{{11}{4}{Confounding of causal effects}{equation.4.11}{}}
\citation{bareinboim_causal_2016}
\citation{anandkumar_learning_2013}
\citation{wang_blessings_2018}
\citation{elidan_ideal_2007}
\citation{gavish_optimal_2014}
\citation{friedman_being_2013}
\citation{elidan_learning_2005}
\newlabel{eq:ceilingTheorem}{{12}{5}{Confounding of causal effects}{equation.4.12}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Implementation}{5}{section.5}}
\citation{scutari_learning_2010}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Learning $\mathaccentV {bar}016{U}$ from structure residuals via EM\relax }}{6}{algocf.1}}
\newlabel{alg:latentEM}{{1}{6}{Implementation}{algocf.1}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {2}{\ignorespaces Inferring linearly optimal $\mathaccentV {bar}016{U}$ and assessing its cardinality by permutations\relax }}{6}{algocf.2}}
\newlabel{alg:linearPCA}{{2}{6}{Implementation}{algocf.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Numerical Demonstration}{6}{section.6}}
\citation{karatzoglou_kernlab_2004}
\citation{louizos_causal_2017}
\citation{kraskov_estimating_2004}
\citation{shepherd_probability-scale_2016}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces True network. \relax }}{7}{figure.caption.5}}
\newlabel{fig_truenet}{{3}{7}{True network. \relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  Estimated network when $U_1$ and $U_2$ are unobserved.\relax }}{7}{figure.caption.5}}
\newlabel{fig_missing}{{4}{7}{Estimated network when $U_1$ and $U_2$ are unobserved.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Estimated network at the last iteration of algorithm \ref  {alg:latentEM}.\relax }}{7}{figure.caption.5}}
\newlabel{fig_estnet_infered}{{5}{7}{Estimated network at the last iteration of algorithm \ref {alg:latentEM}.\relax }{figure.caption.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces $R^2$ in the prediction of the latent variable from the selected principal components.\relax }}{7}{figure.caption.6}}
\newlabel{fig:latrecons}{{6}{7}{$R^2$ in the prediction of the latent variable from the selected principal components.\relax }{figure.caption.6}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Error in coefficients as a function of the iterations.\relax }}{7}{figure.caption.6}}
\newlabel{fig:errorcoef}{{7}{7}{Error in coefficients as a function of the iterations.\relax }{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Gaussian Graphical Models With Interactions}{7}{subsection.6.1}}
\newlabel{eq:residualColumnWithInteractions}{{13}{7}{Gaussian Graphical Models With Interactions}{equation.6.13}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Generalization to nonlinear functions}{7}{subsection.6.2}}
\newlabel{eq:residualColumnRank}{{14}{7}{Generalization to nonlinear functions}{equation.6.14}{}}
\citation{mackay_information_2003}
\citation{elidan_ideal_2007}
\citation{louizos_causal_2017}
\newlabel{resPcaGam}{{15}{8}{Generalization to nonlinear functions}{equation.6.15}{}}
\newlabel{eq:rankSetRelationship}{{16}{8}{Generalization to nonlinear functions}{equation.6.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Generalization to categorical variables}{8}{subsection.6.3}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusions and Future Directions}{8}{section.7}}
\bibdata{LatentVars}
\bibcite{anandkumar_learning_2013}{{1}{2013}{{Anandkumar et~al.}}{{Anandkumar, Hsu, Javanmard, and Kakade}}}
\bibcite{bareinboim_causal_2016}{{2}{2016}{{Bareinboim and Pearl}}{{}}}
\bibcite{damour_multi-cause_2019}{{3}{2019}{{D'Amour}}{{}}}
\bibcite{elidan_learning_2005}{{4}{2005}{{Elidan and Friedman}}{{}}}
\bibcite{elidan_discovering_2001}{{5}{2001}{{Elidan et~al.}}{{Elidan, Lotner, Friedman, and Koller}}}
\bibcite{elidan_ideal_2007}{{6}{2007}{{Elidan et~al.}}{{Elidan, Nachman, and Friedman}}}
\bibcite{friedman1998bayesian}{{7}{1998}{{Friedman}}{{}}}
\bibcite{friedman_being_2013}{{8}{2013}{{Friedman and Koller}}{{}}}
\bibcite{friedman1997learning}{{9}{1997}{{Friedman and {others}}}{{}}}
\bibcite{gavish_optimal_2014}{{10}{2014}{{Gavish and Donoho}}{{}}}
\bibcite{hernan_estimating_2006}{{11}{2006}{{Hern\IeC {\'a}n and Robins}}{{}}}
\bibcite{karatzoglou_kernlab_2004}{{12}{2004}{{Karatzoglou et~al.}}{{Karatzoglou, Smola, Hornik, and Zeileis}}}
\bibcite{koller_probabilistic_2009}{{13}{2009}{{Koller and Friedman}}{{}}}
\bibcite{kraskov_estimating_2004}{{14}{2004}{{Kraskov et~al.}}{{Kraskov, St\IeC {\"o}gbauer, and Grassberger}}}
\bibcite{louizos_causal_2017}{{15}{2017}{{Louizos et~al.}}{{Louizos, Shalit, Mooij, Sontag, Zemel, and Welling}}}
\bibcite{mackay_information_2003}{{16}{2003}{{MacKay}}{{}}}
\bibcite{pearl_causality:_2000}{{17}{2000}{{Pearl}}{{}}}
\bibcite{rawlings_applied_1998}{{18}{1998}{{Rawlings et~al.}}{{Rawlings, Pantula, and Dickey}}}
\bibcite{scutari_learning_2010}{{19}{2010}{{Scutari}}{{}}}
\bibcite{shepherd_probability-scale_2016}{{20}{2016}{{Shepherd et~al.}}{{Shepherd, Li, and Liu}}}
\bibcite{silva_learning_2006}{{21}{2006}{{Silva et~al.}}{{Silva, Scheines, Glymour, and Spirtes}}}
\bibcite{spirtes_causation_1993}{{22}{1993}{{Spirtes et~al.}}{{Spirtes, Glymour, and Scheines}}}
\bibcite{wang_blessings_2018}{{23}{2018}{{Wang and Blei}}{{}}}
\bibstyle{plainnat}
